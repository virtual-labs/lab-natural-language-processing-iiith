After completing this experiment, students will be able to:

1. **Understand Smoothing in N-gram Models**: Explain the need for smoothing in N-gram language models and describe common smoothing techniques.
2. **Apply Add-One Smoothing**: Calculate smoothed bigram probabilities using Add-One (Laplace) Smoothing.
3. **Analyze Sparse Data**: Identify and address the issue of zero-probability N-grams in sparse bigram tables.
4. **Compare Probability Distributions**: Observe and interpret the effects of smoothing on probability distributions in N-gram models.
5. **Practice with Interactive Simulation**: Gain hands-on experience by filling in bigram probability tables and checking answers interactively.

#### Learning Focus

- Apply Add-One Smoothing to bigram tables
- Understand the impact of smoothing on language model probabilities
- Address zero-probability issues in sparse data
- Practice probability calculations in an interactive environment
