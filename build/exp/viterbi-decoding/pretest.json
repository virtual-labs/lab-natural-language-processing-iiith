{
  "version": 2.0,
  "questions": [
    {
      "question": "What is the primary purpose of the Viterbi algorithm in Natural Language Processing?",
      "answers": {
        "a": "To find the most probable sequence of hidden states given observations",
        "b": "To translate text from one language to another",
        "c": "To count word frequencies in a corpus",
        "d": "To generate random sentences"
      },
      "explanations": {
        "a": "Correct! The Viterbi algorithm uses dynamic programming to find the most likely sequence of hidden states (like POS tags) given observable data (like words).",
        "b": "Incorrect. The Viterbi algorithm is not used for machine translation.",
        "c": "Incorrect. The Viterbi algorithm is not used for counting frequencies.",
        "d": "Incorrect. The Viterbi algorithm is deterministic and finds optimal solutions, not random ones."
      },
      "correctAnswer": "a",
      "difficulty": "beginner"
    },
    {
      "question": "In the context of POS tagging, what do the 'hidden states' represent in a Hidden Markov Model?",
      "answers": {
        "a": "The POS tags (noun, verb, adjective, etc.)",
        "b": "The words in the sentence",
        "c": "The punctuation marks",
        "d": "The sentence length"
      },
      "explanations": {
        "a": "Correct! In POS tagging, the hidden states are the grammatical categories (POS tags) that we want to predict.",
        "b": "Incorrect. Words are the observable symbols, not hidden states.",
        "c": "Incorrect. Punctuation marks are not typically the focus of basic POS tagging.",
        "d": "Incorrect. Sentence length is not a hidden state in this context."
      },
      "correctAnswer": "a",
      "difficulty": "beginner"
    },
    {
      "question": "What information does the emission matrix provide in HMM-based POS tagging?",
      "answers": {
        "a": "P(word | POS tag) - the probability of observing a word given a POS tag",
        "b": "P(POS tag | word) - the probability of a POS tag given a word",
        "c": "The frequency of words in the training corpus",
        "d": "The order of words in a sentence"
      },
      "explanations": {
        "a": "Correct! The emission matrix contains emission probabilities P(word|tag), showing how likely each word is to be generated by each POS tag.",
        "b": "Incorrect. This would be the inverse relationship, which is not what the emission matrix directly represents.",
        "c": "Incorrect. While frequencies are used to compute probabilities, the emission matrix contains probabilities, not raw frequencies.",
        "d": "Incorrect. Word order is handled by transition probabilities, not emission probabilities."
      },
      "correctAnswer": "a",
      "difficulty": "beginner"
    },
    {
      "question": "What does the transition matrix represent in the context of POS tagging?",
      "answers": {
        "a": "P(tag_i | tag_i-1) - the probability of transitioning from one POS tag to another",
        "b": "P(word_i | word_i-1) - the probability of one word following another",
        "c": "The total number of tags in the tagset",
        "d": "The length of the training sentences"
      },
      "explanations": {
        "a": "Correct! The transition matrix contains probabilities of moving from one POS tag to the next, capturing syntactic patterns in language.",
        "b": "Incorrect. The transition matrix deals with tag-to-tag transitions, not word-to-word transitions.",
        "c": "Incorrect. The matrix contains probabilities, not counts of tags.",
        "d": "Incorrect. Sentence length is not what the transition matrix represents."
      },
      "correctAnswer": "a",
      "difficulty": "intermediate"
    },
    {
      "question": "What is the key computational technique that makes the Viterbi algorithm efficient?",
      "answers": {
        "a": "Dynamic programming",
        "b": "Brute force search",
        "c": "Random sampling",
        "d": "Greedy search"
      },
      "explanations": {
        "a": "Correct! Dynamic programming allows the Viterbi algorithm to avoid redundant calculations by storing and reusing intermediate results, reducing complexity from exponential to polynomial time.",
        "b": "Incorrect. Brute force would be exponentially slow and impractical for sequence labeling.",
        "c": "Incorrect. Random sampling doesn't guarantee finding the optimal solution.",
        "d": "Incorrect. While the Viterbi algorithm makes locally optimal choices, it's specifically a dynamic programming approach."
      },
      "correctAnswer": "a",
      "difficulty": "intermediate"
    },
    {
      "question": "In the Viterbi algorithm, what does each cell V[i][j] in the Viterbi table represent?",
      "answers": {
        "a": "The maximum probability of any tag sequence ending in tag i at word position j",
        "b": "The emission probability of word j given tag i",
        "c": "The transition probability from tag i to tag j",
        "d": "The frequency of tag i at position j in the training data"
      },
      "explanations": {
        "a": "Correct! Each cell stores the highest probability of any path that ends with tag i at word position j, which is the core of the dynamic programming approach.",
        "b": "Incorrect. That would be the emission matrix, not the Viterbi table cell.",
        "c": "Incorrect. That would be the transition matrix, not the Viterbi table cell.",
        "d": "Incorrect. The Viterbi table contains computed probabilities, not training frequencies."
      },
      "correctAnswer": "a",
      "difficulty": "intermediate"
    },
    {
      "question": "What is the time complexity of the Viterbi algorithm for a sentence of length N with T possible tags?",
      "answers": {
        "a": "O(N × T²)",
        "b": "O(T^N)",
        "c": "O(N × T)",
        "d": "O(N²)"
      },
      "explanations": {
        "a": "Correct! For each of the N words, we consider T tags, and for each tag we look at T previous tags, giving O(N × T²) complexity.",
        "b": "Incorrect. This would be the complexity of exhaustive search without dynamic programming (T choices for each of N positions).",
        "c": "Incorrect. This doesn't account for the need to consider all previous tags when computing each cell.",
        "d": "Incorrect. This doesn't account for the number of possible tags."
      },
      "correctAnswer": "a",
      "difficulty": "advanced"
    },
    {
      "question": "In the Viterbi algorithm, why is backtracking necessary after filling the table?",
      "answers": {
        "a": "To reconstruct the optimal tag sequence by following the path of maximum probabilities",
        "b": "To verify that all probabilities sum to 1",
        "c": "To calculate the emission and transition matrices",
        "d": "To count the total number of possible tag sequences"
      },
      "explanations": {
        "a": "Correct! The forward pass computes maximum probabilities, but we need backtracking to trace back through the stored pointers to recover the actual sequence of tags that achieved these probabilities.",
        "b": "Incorrect. Probability normalization is not the purpose of backtracking in Viterbi.",
        "c": "Incorrect. The matrices are computed from training data before running Viterbi.",
        "d": "Incorrect. Counting sequences is not the goal; finding the best sequence is."
      },
      "correctAnswer": "a",
      "difficulty": "advanced"
    },
    {
      "question": "Consider a sentence 'Book a park' where 'Book' could be either a noun or verb. How does the Viterbi algorithm handle this ambiguity?",
      "answers": {
        "a": "It considers both possibilities and chooses based on the combination of emission and transition probabilities throughout the sequence",
        "b": "It always chooses the most frequent tag for the word",
        "c": "It ignores the context and assigns tags randomly",
        "d": "It only looks at the emission probability for that word"
      },
      "explanations": {
        "a": "Correct! The Viterbi algorithm evaluates all possible tag combinations and selects the sequence that maximizes the overall probability considering both local word-tag fit and global sequence coherence.",
        "b": "Incorrect. Frequency alone doesn't account for context and sequence constraints.",
        "c": "Incorrect. The Viterbi algorithm is deterministic and heavily relies on context.",
        "d": "Incorrect. Both emission and transition probabilities are crucial for optimal decisions."
      },
      "correctAnswer": "a",
      "difficulty": "advanced"
    }
  ]
}
