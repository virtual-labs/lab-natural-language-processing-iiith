{
  "version": 2.0,
  "questions": [
    {
      "question": "What is a 'chunk' in the context of Natural Language Processing?",
      "answers": {
        "a": "A non-overlapping, meaningful group of words such as a noun or verb phrase",
        "b": "A single word only",
        "c": "A random group of words",
        "d": "A full syntactic tree"
      },
      "explanations": {
        "a": "Correct! A chunk is a non-overlapping, non-recursive group of words forming a meaningful unit, like a noun phrase.",
        "b": "Incorrect. Chunks can be more than one word.",
        "c": "Incorrect. Chunks are not random groups, but linguistically meaningful units.",
        "d": "Incorrect. Chunks are not full syntactic trees."
      },
      "correctAnswer": "a",
      "difficulty": "beginner"
    },
    {
      "question": "Which of the following is NOT typically used as a feature for chunking?",
      "answers": {
        "a": "Word itself (lexicon)",
        "b": "Part-of-speech (POS) tag",
        "c": "Font color",
        "d": "Neighboring POS tags"
      },
      "explanations": {
        "a": "Incorrect. The word itself is a common feature.",
        "b": "Incorrect. POS tags are essential features for chunking.",
        "c": "Correct! Font color is not a linguistic feature and is not used in chunking.",
        "d": "Incorrect. Neighboring POS tags are often used as features."
      },
      "correctAnswer": "c",
      "difficulty": "beginner"
    },
    {
      "question": "What is the main advantage of using both lexicon and POS features for chunking?",
      "answers": {
        "a": "It provides richer information, improving chunking accuracy",
        "b": "It makes the model slower",
        "c": "It reduces the number of chunks",
        "d": "It is required for all languages"
      },
      "explanations": {
        "a": "Correct! Combining lexical and POS features gives the model more context, usually resulting in higher accuracy.",
        "b": "Incorrect. While more features may increase computation, the main advantage is improved accuracy.",
        "c": "Incorrect. The number of chunks depends on the sentence, not the features.",
        "d": "Incorrect. While helpful, it is not strictly required for all languages."
      },
      "correctAnswer": "a",
      "difficulty": "beginner"
    },
    {
      "question": "If you increase the size of the training corpus for a chunker, what is the most likely effect?",
      "answers": {
        "a": "Accuracy increases (up to a point)",
        "b": "Accuracy decreases",
        "c": "No effect on accuracy",
        "d": "The model stops working"
      },
      "explanations": {
        "a": "Correct! More training data usually helps the model generalize better, improving accuracy until a saturation point.",
        "b": "Incorrect. More data usually helps, not hurts, accuracy.",
        "c": "Incorrect. Training data size does affect accuracy.",
        "d": "Incorrect. The model does not stop working with more data."
      },
      "correctAnswer": "a",
      "difficulty": "intermediate"
    },
    {
      "question": "Which model is generally more flexible for chunking: HMM or CRF?",
      "answers": {
        "a": "HMM",
        "b": "CRF",
        "c": "Both are equally flexible",
        "d": "Neither can be used for chunking"
      },
      "explanations": {
        "a": "Incorrect. HMMs are less flexible in feature representation.",
        "b": "Correct! CRFs can use a wider range of features and dependencies, making them more flexible for chunking.",
        "c": "Incorrect. CRFs are generally more flexible than HMMs.",
        "d": "Incorrect. Both can be used for chunking."
      },
      "correctAnswer": "b",
      "difficulty": "intermediate"
    },
    {
      "question": "What is a common error type in chunking output?",
      "answers": {
        "a": "Boundary error (incorrect chunk start/end)",
        "b": "Font error",
        "c": "POS tag error only",
        "d": "No errors occur in chunking"
      },
      "explanations": {
        "a": "Correct! Boundary errors, where the chunk start or end is misplaced, are common in chunking tasks.",
        "b": "Incorrect. Font is not relevant to chunking errors.",
        "c": "Incorrect. While POS errors can affect chunking, boundary errors are more specific to chunking.",
        "d": "Incorrect. Errors do occur in chunking."
      },
      "correctAnswer": "a",
      "difficulty": "intermediate"
    },
    {
      "question": "Suppose your chunker is making many mistakes. Which of the following is LEAST likely to help?",
      "answers": {
        "a": "Adding more labeled training data",
        "b": "Using richer features (lexicon + POS)",
        "c": "Randomly shuffling the training labels",
        "d": "Switching from HMM to CRF"
      },
      "explanations": {
        "a": "Incorrect. More labeled data usually helps.",
        "b": "Incorrect. Richer features usually help.",
        "c": "Correct! Randomly shuffling the labels will harm, not help, accuracy.",
        "d": "Incorrect. Switching to a more flexible model like CRF can help."
      },
      "correctAnswer": "c",
      "difficulty": "advanced"
    },
    {
      "question": "What does the 'Check Accuracy' step in the simulation demonstrate?",
      "answers": {
        "a": "How well the chunker performs on unseen data",
        "b": "How to count the number of words in a sentence",
        "c": "How to translate a sentence",
        "d": "How to generate random sentences"
      },
      "explanations": {
        "a": "Correct! The accuracy check shows the chunker's performance on test data, reflecting its generalization ability.",
        "b": "Incorrect. The step is not about word counting.",
        "c": "Incorrect. The step is not about translation.",
        "d": "Incorrect. The step is not about sentence generation."
      },
      "correctAnswer": "a",
      "difficulty": "advanced"
    },
    {
      "question": "Which of the following best describes the difference between chunking and full parsing?",
      "answers": {
        "a": "Chunking finds non-overlapping groups, full parsing finds complete syntactic structure",
        "b": "Chunking is slower than full parsing",
        "c": "Full parsing ignores word order",
        "d": "Chunking is only used for English"
      },
      "explanations": {
        "a": "Correct! Chunking identifies non-overlapping, non-recursive groups (chunks), while full parsing builds a complete syntactic tree.",
        "b": "Incorrect. Chunking is usually faster than full parsing.",
        "c": "Incorrect. Full parsing is sensitive to word order.",
        "d": "Incorrect. Chunking is used for many languages."
      },
      "correctAnswer": "a",
      "difficulty": "advanced"
    },
    {
      "question": "In practical NLP applications, why is chunking useful?",
      "answers": {
        "a": "It helps identify meaningful phrases for downstream tasks like information extraction",
        "b": "It replaces the need for all other NLP tasks",
        "c": "It is only used for text generation",
        "d": "It is not useful in practice"
      },
      "explanations": {
        "a": "Correct! Chunking helps identify phrases (like noun or verb phrases) that are useful for tasks such as information extraction, question answering, and more.",
        "b": "Incorrect. Chunking is one of many useful NLP tasks.",
        "c": "Incorrect. Chunking is not limited to text generation.",
        "d": "Incorrect. Chunking is widely used in practice."
      },
      "correctAnswer": "a",
      "difficulty": "advanced"
    }
  ]
}