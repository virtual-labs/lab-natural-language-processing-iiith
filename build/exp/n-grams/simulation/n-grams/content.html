<!-- This file needs to be edited by the lab developer to suit
the requirements of their lab in particular.--><!-- Add class="default" to include any element as it is
specified in default.html. 
Do not include class="default" to the elements that you want to
edit --><!DOCTYPE html>
<html><head><!--Google Tag Manager--><script class="gtm">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-W59SWTR');</script><!--End Google Tag Manager--></head>
<body><!--Google Tag Manager (noscript)--><noscript class="gtm"><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-W59SWTR" style="display:none;visibility:hidden" width="0"></iframe></noscript><!--End Google Tag Manager (noscript)-->

<div id="experiment"> <!-- The Experiment Document Container-->

  <!-- The lab Header contains the logo and the name of the lab,
  usually displayed on the top of the page-->

  <header class="default" id="experiment-header">
  
    <div class="logo" id="experiment-header-logo">
      <!-- Enclose the logo image of your lab or write it in 
      text-->
      <img src="../images/logo.jpg"/>
    </div>

    <div class="heading" id="experiment-header-heading">
      <!-- Write the name of your lab and link it to the home 
      page of your lab (h1 tag is preferred while writing your 
      lab name)-->
      <a href="../index.html">Natural Language Processing Lab</a>	
    </div>

    <!-- Add any additional element you want to add to the lab 
    header, For example : Help (Enclosing them with suitable 
    div is recommended)-->

  </header>


  <!-- The lab article is the main content area where all the 
  experiment content sits-->
  <article id="experiment-article">
  
    <!-- The lab article has an header, optional navigational 
    menu, number of sections, an optional sidebar and a closing 
    footer-->
     <div class="breadcrumb" id="experiment-article-breadcrumb">
     </div>
    
      <header class="heading" id="experiment-article-heading">
        <!-- You can add a welcome message or title of the 
        experiment here -->
        N-Grams
        <!-- Add any additional element if required with proper 
        enclosing-->
      </header>

      <!-- Navigation menu is useful to organize the view of 
      multiple sections inside the article-->
      <nav class="default" id="experiment-article-navigation">
        <ul id="experiment-article-navigation-menu">
          <!-- The menu can be dynamically generated to contain 
          the headings of your sections or instead write the 
          menu items of your choice individually enclosedu in 
          <li> tag as shown below-->
        </ul>
      </nav>

      <!-- All the sections of your lab or experiment can be 
      enclosed together with a div element as shown below-->
      <div id="experiment-article-sections">

        <!-- First section of the article-->
        <section id="experiment-article-section-1">
          
          <div class="icon" id="experiment-article-section-1-icon">
	    <!-- Enclose the icon image of your lab -->
	    <img src="../images/introduction.jpg"/>
	  </div>	
          
          <!-- The heading for the section can be enclosed in a 
          div tag. -->
          <div class="heading" id="experiment-article-section-1-heading">
            Introduction
          </div>

          <!-- Write the section content inside a paragraph 
          element, You can also include images with <img> tag -->
          <div class="content" id="experiment-article-section-1-content">	
Probability of a sentence can be calculated by the probability of sequence of words occuring in it. We can use Markov assumption, that the probability of a word in a sentence depends on the probability of the word occuring just before it. Such a model is called first order Markov model or the bigram model. <br/>

<center><img alt="1_alt" src="Exp9/9-a.jpg" style="height:50px; width:400px"/></center><br/>

Here, W<sub>n</sub> refers to the word token corresponding to the nth word in a sequence.   
<br/><br/><hr/>
        </div>


      </section>

      <!-- Second section of the article-->
      <section id="experiment-article-section-2">
        
        <div class="icon" id="experiment-article-section-2-icon">
	  <!-- Enclose the icon image of your lab. -->
	  <img src="../images/theory.jpg"/>
	</div>
				
        <!-- The heading for the section can be enclosed in a 
        div tag. -->
        <div class="heading" id="experiment-article-section-2-heading">
          Theory
        </div>


        <!-- Write the section content inside a paragraph 
        element, we can also include images with <img> tag -->
<div class="content" id="experiment-article-section-2-content">
A combination of words forms a sentence. However, such a formation is meaningful  only when the words are arranged in some order. <br/>
Eg:  Sit I car in the<br/>
Such a sentence is not grammatically acceptable. However some perfectly grammatical sentences can be nonsensical too!<br/>
Eg: Colorless green ideas sleep furiously<br/>
One easy way to handle such unacceptable sentences is by assigning probabilities to the strings of words i.e, how likely the sentence is. <br/>
<br/>
<b><u> Probability of a sentence </u></b><br/>

If we consider each word occurring in its correct location as an independent event,the probability of the sentences is : P(w(1), w(2)..., w(n-1), w(n))<br/><br/>
Using chain rule:<br/>
=<b>P(</b>w(1)<b>)</b> * <b>P(</b>w(2) | w(1)<b>)</b> * <b>P(</b>w(3) | w(1)w(2)<b>)</b> ... <b>P(</b>w(n) | w(1)w(2)…w(n-1)<b>)</b><br/><br/>
<b><u> Bigrams</u></b><br/>
We can avoid this very long calculation by approximating  that the probability of a given word depends only on the probability of its previous words. This assumption is called Markov assumption and such a model is called Markov model- bigrams. Bigrams can be generalized to the n-gram which looks at (n-1) words in the past. A bigram is a first-order Markov model. <br/>

Therefore , <br/>

     <b>P(</b>w(1), w(2)..., w(n-1), w(n)<b>)</b>= <b>P(</b>w(2)|w(1)<b>)</b> <b>P(</b>w(3)|w(2)<b>)</b> …. <b>P(</b>w(n)|w(n-1)<b>)</b><br/>
<br/>
We use (eos) tag to mark the beginning and end of a sentence.<br/>
A bigram table for a given corpus can be generated and used as a lookup table for calculating probability of sentences.<br/><br/>

Eg:
 
Corpus – (eos) You book a flight (eos) I read a book (eos) You read (eos) <br/><br/>

Bigram Table:<br/>
<table border="1">
<tbody><tr><th></th><th>(eos)</th><th> you </th> <th> book </th> <th> a </th> <th> flight </th> <th> I </th> <th> read </th> </tr>
<tr>
</tr><tr>
<th> (eos)</th><td> 0</td><td> 0.33</td><td>0</td><td> 0 </td> <td> 0 </td> <td> 0.25 </td><td> 0 </td> </tr>
<tr>
<th>you</th><td>0</td><td> 0 </td><td>0.5</td><td> 0 </td> <td> 0 </td> <td> 0</td> <td>0.5</td></tr>
<tr>
<th>book</th><td> 0.5 </td><td> 0</td><td>0</td><td> 0.5 </td> <td> 0 </td> <td> 0</td> <td>0</td></tr>
<tr>
<th>a</th><td> 0 </td><td> 0</td><td>0.5</td><td> 0 </td> <td> 0.5 </td> <td> 0</td> <td>0</td> </tr>  
<tr>
<th>flight</th><td> 1 </td><td> 0</td><td>0</td><td> 0 </td> <td> 0 </td> <td> 0</td> <td>0</td> </tr> 
<tr>
<th>I</th><td> 0 </td><td> 0</td><td>0</td><td> 0 </td> <td> 0 </td> <td> 0</td> <td>1</td> </tr>
<tr>
<th>read</th><td> 0.5 </td><td> 0</td><td>0</td><td> 0.5 </td> <td> 0 </td> <td> 0</td> <td>0</td> </tr> 
</tbody></table> 
 
<br/>
<br/>
<b>P(</b>(eos) you read a book (eos)<b>)</b><br/>
= <b>P(</b>you|eos<b>)</b> * <b>P(</b>read|you<b>)</b> * <b>P(</b>a|read<b>)</b> * <b>P(</b>book|a<b>)</b> * <b>P(</b>eos|book<b>)</b><br/>
= 0.33 * 0.5 * 0.5 * 0.5 * 0.5<br/>
=.020625<br/>
</div>
      </section>


      <section id="experiment-article-section-3">
        
        <div class="icon" id="experiment-article-section-3-icon">
	  <!-- Enclose the icon image of your lab. -->
	  <img src="../images/objective.jpg"/>
	</div>
     
        <div class="heading" id="experiment-article-section-3-heading">
          Objective
        </div>

        <div class="content" id="experiment-article-section-3-content">

The objective of this experiment is to learn to calculate bigrams from a given corpus and calculate probability of a sentence.

<br/><br/><hr/>

        </div>

      </section>


      <section id="experiment-article-section-4">

        <div class="icon" id="experiment-article-section-4-icon">
	  <!-- Enclose the icon image of your lab.-->
	  <img src="../images/simulation.jpg"/>
	</div>

        <div class="heading" id="experiment-article-section-4-heading">
          Experiment
        </div>

        <div class="content" id="experiment-article-section-4-content">
<div id="n-gram"></div>
        </div>

      </section>


        <section id="experiment-article-section-5">
      
          <div class="icon" id="experiment-article-section-5-icon">
	    <!-- Enclose the icon image of your lab.-->
	    <img src="../images/quizzes.jpg"/>
	  </div>

          <div class="heading" id="experiment-article-section-5-heading">
            Quizzes
          </div>

          <div class="content" id="experiment-article-section-5-content">
<b><u> Questions </u></b><br/><br/>
Q1. A trigram is a second-order Markov model. Derive the formula to calculate trigram probability. Next, calculate the trigram probabilities for the given corpus.
<center>(eos) Can I sit near you (eos) You can sit (eos) Sit near him (eos) I can sit you (eos) </center>
<br/><br/>
Q2. A character based N-gram is a set of n consecutive characters extracted from a word. It is generally used in measuring the similarity of character strings. Some of its applications are in spellcheckers, stemming, OCR error correction, etc. <br/><br/>

Given, four valid words:<br/>
(a) quote<br/>
(b) patient<br/>
(c) patent<br/>
(d) impatient<br/><br/>

Calculate the probability of occurrence of each word given below. Which of these represent the correct spelling? <br/>
(a) qotient (b) quotent (c) quotient<br/>
<br/><br/><hr/>
</div>
        </section>

        <section id="experiment-article-section-6">
	  
          <div class="icon" id="experiment-article-section-6-icon">
	    <!-- Enclose the icon image of your lab. -->
	    <img src="../images/procedure.jpg"/>
	  </div>
	
          <div class="heading" id="experiment-article-section-6-heading">
	    Procedure
	  </div>
	
          <div class="content" id="experiment-article-section-6-content">
<b><u>STEP1: </u></b>Select a corpus  and click on <button>Generate bigram table</button><br/>
<b><u>STEP2: </u></b>Fill up the table that is generated and hit <button>Submit</button><br/>
<b><u>STEP3: </u></b>If incorrect (red), see the correct answer by clicking on show answer or repeat Step 2.<br/>
<b><u>STEP4: </u></b>If correct (green), click on take a quiz and fill the correct answer
<br/><br/>
<hr/>
	  </div>
	
        </section>
			
		
        <section id="experiment-article-section-7">
   
          <div class="icon" id="experiment-article-section-7-icon">
	    <!-- Enclose the icon image of your lab.-->
	    <img src="../images/readings.jpg"/>
	  </div>

          <div class="heading" id="experiment-article-section-7-heading">
            Further Readings
          </div>

          <div class="content" id="experiment-article-section-7-content">
<p></p><center><b>Speech and Language Processing - <i>An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition</i></b><br/>
BY: Daniel Jurafsky and James H. Martin<br/>
<i>Chapter 6</i></center><p></p>
<br/>
<br/>
<hr/>
          </div>

        </section>

      </div>


    <!-- An article can have a sidebar that contain related 
    links and additional material (however it is kept optional 
    at this moment) -->
    <aside class="default" id="lab-article-sidebar">
      <!-- put the content that you want to appear in the 
      sidebar -->	
    </aside>


    <!-- Article footer can display related content and 
    additional links -->						
    <footer class="default" id="footer"><div class="footer-top" id="contact">
  <div class="container">
    <div class="row">
      <div class="col-lg-4 col-md-6">
        <h4>Community Links</h4>
        <p>
	  <a href="http://www.sakshat.ac.in/">Sakshat Portal</a>
	</p>
        <p>
	  <a href="http://outreach.vlabs.ac.in/">Outreach Portal</a>
	</p>
        <p>
	  <a href="http://vlab.co.in/faq">FAQ : Virtual Labs</a>
	</p>
      </div>
      <div class="col-lg-4 col-md-6">
        <h4>Contact Us</h4>
	<p> <strong>Phone:</strong> General Information : 011-26582050 </p>
	<p> <strong>Email:</strong> support@vlab.co.in </p>
      </div>
      <div class="col-lg-4 col-md-6">
	<h4>Follow Us</h4>
	<div class="social-links">
	  <a class="twitter" href="https://twitter.com/TheVirtualLabs" style="background: #55acee;">
	    <i class="fa fa-twitter"></i>
	  </a>
	  <a class="facebook" href="https://www.facebook.com/Virtual-Labs-IIT-Delhi-301510159983871/" style="background: #3b5998;">
	    <i class="fa fa-facebook"></i>
	  </a>
	  <a class="google-plus" href="https://www.youtube.com/watch?v=asxRaOgk6a0" style="background: #e52d27;">
	    <i class="fa fa-youtube"></i>
	  </a>
	  <a class="linkedin" href="https://in.linkedin.com/in/virtual-labs-008ba9136" style="background: #2867B2;">
	    <i class="fa fa-linkedin"></i>
	  </a>
	</div>
      </div>
    </div>
  </div>
</div>
</footer>

  </article>


  <!-- Links to other labs, about us page can be kept the lab 
  footer-->
  <footer class="default" id="lab-footer">
    <!-- Put the content here-->
  </footer>

</div>		



</body></html>